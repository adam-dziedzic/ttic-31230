\documentclass{article}
\input ../preamble
\parindent = 0em

\newcommand{\solution}[1]{}
%\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}

\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}
\bigskip
\centerline{\bf Generalization Problems}

\bigskip
\bigskip

{\bf Problem 1.} Consider the regularized objective
$$\Phi^* = \argmin_\Phi \;E_{(x,y) \sim \mathrm{Train}}\;\left({\cal L}(\Phi,x,y) + \frac{\lambda}{2N}||\Phi||^2\right)$$
By setting the gradient of the objective to zero, solve for the average gradient $g$ as a function of $\Phi^*$.

\medskip
{\bf Problem 2.} Consider any probability distribution $P(h)$ over an discrete class ${\cal H}$.
Assume $0 \leq {\cal L}(h,x,y) \leq \lmax$. Define
\begin{eqnarray*}
{\cal L}(h)  & = &  E_{(x,y)\sim \mathrm{Pop}}\;{\cal L}(h,x,y) \\
\\
\hat{{\cal L}}(h) & = & E_{(x,y)\sim \mathrm{Train}}\;{\cal L}(h,x,y)
\end{eqnarray*}
We now have the theorem that with probability
at least $1-\delta$ over the draw of training data the following holds simultaneously for all $h$.
$${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{\ln \frac{1}{P(h)} + \ln\frac{1}{\delta}}} \;\;\;(1)$$

This motivates
$$h^* = \argmin_h \;\hat{{\cal L}}(h) + \frac{5\lmax}{N} \ln\frac{1}{P(h)}\;\;\;\;(2)$$
The Bayesian maximum a-posteriori (MAP) rule is
$$h^* = \argmax_h\;P(h)\prod_{(x,y) \in \mathrm{Train}}\;P(y|x,h)\;\;\;\;(3)$$
For ${\cal L}(h,x,y) = - \ln P(y|x,h)$ (cross entropy loss) rewrite (2) so as to be as similar to (3) as possible.
Note that (1) holds independent of any ``truth'' of the ``prior'' $P$.

\bigskip
{\bf Problem 3.}

\medskip
(a) Consider a model where the parameter vector $\Phi$ has $d$ parameters each of which is represented by a 16 bit floating point number.
Express the bound (1) in problem 2 in terms of the dimension $d$ assuming all parameter vectors are equally likely.

\medskip
(b) Define a probability distribution over variable precision floating point representations where any number of bits of mantissa is possible and any number of bits of exponent
is possible and then express the bound (1) in terms of this variable-precision representation of numbers.
{\bf Problem 2 (25 points).} Consider any probability distribution
$P(h)$ over an discrete class ${\cal H}$.  Assume $0 \leq {\cal
  L}(h,x,y) \leq \lmax$. Define
\begin{eqnarray*}
{\cal L}(h) & = & E_{(x,y)\sim \mathrm{Pop}}\;{\cal L}(h,x,y)
\\ \hat{{\cal L}}(h) & = & E_{(x,y)\sim \mathrm{Train}}\;{\cal
  L}(h,x,y)
\end{eqnarray*}
We now have the theorem that with probability at least $1-\delta$ over
the draw of training data the following holds simultaneously for all
$h$.
$${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{\ln \frac{1}{P(h)} + \ln\frac{1}{\delta}}} \;\;\;(1)$$

This motivates
$$h^* = \argmin_h \;\hat{{\cal L}}(h) + \frac{5\lmax}{N} \ln\frac{1}{P(h)}\;\;\;\;(2)$$
The Bayesian maximum a-posteriori (MAP)
rule is
$$h^* = \argmax_h\;P(h)\prod_{(x,y) \in  \mathrm{Train}}\;P(y|x,h)\;\;\;\;(3)$$
For ${\cal L}(h,x,y) = - \ln P(y|x,h)$ (cross entropy loss) rewrite (2) so as to be as similar to
(3) as possible.  Keep in mind that
$$\hat{\cal L}(h) = \frac{1}{N} \sum_{(x,y) \in \mathrm{Train}} -\ln P(y|x,h)$$

\solution{
  \begin{eqnarray*}
    & & \argmin_h \;\left(\frac{1}{N} \sum_{(x,y)\sim \mathrm{Train}}\;-\ln P(y|x,h)\right) + \frac{5\lmax}{N} \ln\frac{1}{P(h)} \\
    \\
    & =  & \argmax_h \;\left(\frac{1}{N} \sum_{(x,y)\sim \mathrm{Train}}\;\ln P(y|x,h)\right) + \frac{5\lmax}{N} \ln P(h) \\
    \\
    & =  & \argmax_h \left(\sum_{(x,y)\sim \mathrm{Train}}\;\ln P(y|x,h)\right) + 5\lmax \ln P(h) \\
    \\
    & =  & \argmax_h \ln \left(P(h)^{5\lmax}\prod_{(x,y)\sim \mathrm{Train}}\; P(y|x,h)\right) \\
    \\
    & = & \argmax_h\;  P(h)^{5\lmax}\prod_{(x,y)\sim \mathrm{Train}}\; P(y|x,h)
  \end{eqnarray*}
}


\eject {\bf Problem 3 (25 points).}

\medskip
(a) Consider a model with $d$ parameters each of which is represented
by a 32 bit floating point number.  Express the bound (1) in problem 2
in terms of the dimension $d$ assuming all representable parameter
vectors are equally likely.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{32d\ln 2 + \ln\frac{1}{\delta}}}$$
}

\medskip
(b) Repeat part (a) but for a model with $d$ parameters represented by
$\Phi_i = z[J[i]]$ where $J[i]$ is an integer index with $0 \leq J[i]
< 32$ and where $z[j]$ is a 32 bit floating point number and where all parameter vectors are equally likely.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{(32^2 + 5d)\ln 2 + \ln\frac{1}{\delta}}}$$
}

\end{document}
