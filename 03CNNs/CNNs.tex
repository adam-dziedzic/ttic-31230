\input ../SlidePreamble
\input ../preamble

\begin{document}

{\Huge

  \centerline{\bf TTIC 31230, Fundamentals of Deep Learning}
  \bigskip
  \centerline{David McAllester, Winter 2020}

    \vfill
  \centerline{\bf Convolutional Neural Networks (CNNs)}
  \vfill
  \vfill


\slide{Imagenet Classification}

1000 kinds of objects.

\vfill
\centerline{\includegraphics[height=4.2in]{../images/IVLSRC}}
2016 is 3.0\%, is 2017 2.25\%

\medskip
SOTA as of January 2020 is 1.3\%

\slidetwoplain{What is a CNN?}
{VGG, Zisserman, 2014}

\centerline{\includegraphics[width = 8.0in]{../images/VGG}}
\centerline{\large Davi Frossard}


\slideplain{A Convolution Layer}
\centerline{\includegraphics[height = 2.0in]{../images/VGG}}

\vfill
Each box is a tensor $L_\ell[b,x,y,i]$

\vfill
For a convolution layer, each $L_{{\ell+1}}[b,x,y,j]$ is the output of a single linear threshold unit computed from $L_\ell[b,x,y,i]$.

\slideplain{A Convolution Layer}

\centerline{\includegraphics[width = 2.5in]{../images/Convolution}}
\centerline{$W[\Delta x,\Delta y,i,j]$\hspace{6ex}$L_{{\ell}}[b,x,y,i]$\hspace{6ex}$L_{{\ell+1}}[b,x,y,j]$}
\centerline{\large River Trail Documentation}

\vfill
\begin{eqnarray*}
 & &  L_{{\ell+1}}[b,x,y,j] \\
 \\
 & = &   \sigma\left(\left(\sum_{\Delta x, \Delta y, i}\;W[\Delta x, \Delta y, i,j]\; L_{{\ell}}[b,x + \Delta x, y + \Delta y, i]\right) - B[j]\right)
\end{eqnarray*}

\slide{Many ``Neurons'' (Linear Threshold Units)}

Each $L_{{\ell+1}}[b,x,y,j]$ is the output of a single linear threshold unit.

\bigskip
\begin{eqnarray*}
 & &  L_{{\ell+1}}[b,x,y,j] \\
 \\
 & = &   \sigma\left(\left(\sum_{\Delta x, \Delta y, i}\;W[\Delta x, \Delta y, i,j]\; L_{{\ell}}[b,x + \Delta x, y + \Delta y, i]\right) - B[j]\right)
\end{eqnarray*}

\slide{2D CNN in PyTorch}

conv2d({\bf input, weight, bias, stride, padding, dilation, groups})

\bigskip
{\bf input} – tensor (minibatch,in-channels,iH,iW)

\medskip
{\bf weight} – filters (out-channels, in-channels/groups, in-channels,kH,kW)

\medskip
{\bf bias} – tensor (out-channels) . Default: None

\medskip
{\bf stride} – Single number or (sH, sW). Default: 1

\medskip
{\bf padding} – Single number or (padH, padW). Default: 0

\medskip
{\bf dilation} – Single number or (dH, dW). Default: 1

\medskip
{\bf groups} – split input into groups. Default: 1

\slide{Padding}

\centerline{\includegraphics[height = 2.0in]{../images/padding2}}
\centerline{\large Jonathan Hui}

\vfill
If we pad the input with zeros then the input and output can have the same spatial dimensions.

\slide{Zero Padding in NumPy}

In NumPy we can add a zero padding of width p to an image as follows:

\vfill
\begin{verbatim}
padded = np.zeros(W + 2*p,  H + 2*p)

padded[p:W+p, p:H+p] = x
\end{verbatim}

\slide{Padding}

$L'_{{\ell}} = \mathrm{Padd}(L_{{\ell}},\;p)$

\vfill
$L_{{\ell+1}}[b,x,y,j] =$

\vfill
$\;\;\;\sigma\left(\left(\sum_{\Delta x, \Delta y, i}\;W[\Delta x, \Delta y, i,j]\;L'_{{\ell}}[b,x + \Delta x, y + \Delta y, i]\right) - B[j] \right)$

\vfill
If the input is padded but the output is not padded then $\Delta x$ and $\Delta y$ are non-negative.

\slide{Reducing Spatial Dimention}

\centerline{\includegraphics[width = 8.0in]{../images/VGG}}


\slide{Reducing Spatial Dimensions: Max Pooling}

\centerline{\includegraphics[width = 2.5in]{../images/Convolution}}

\vfill
$$L_{{\ell+1}}[b,{\color{red}x},{\color{red}y},i] = {\color{red} \max_{\Delta x, \Delta y}}\; L_{{\ell}}[b,{\color{red} s*x} + \Delta x,\; {\color{red} s*y} + \Delta y,\; i]$$

\vfill
This is typically done with a stride greater than one so that the image dimension is reduced.

\slide{Reducing Spatial Dimensions: Strided Convolution}

We can move the filter by a ``stride'' $s$ for each spatial step.

\vfill
$L_{{\ell+1}}[b,{\color{red}x},{\color{red}y},j] =$

$$\hspace{-1em}\sigma\left(\left(\sum_{\Delta x,\Delta y,i}\;W[\Delta x, \Delta y, i, j] L_{{\ell}}[b,{\color{red} s*x} + \Delta x, {\color{red} s*y} + \Delta y, i]\right) - B[j]\right)$$

\vfill
For strides greater than 1 the spatial dimention is reduced.


\slide{Fully Connected (FC) Layers}

\centerline{\includegraphics[width = 8.0in]{../images/VGG}}


\slide{Fully Connected (FC) Layers}

We reshape $L_{{\ell}}[b,x,y,i]$ to $L_{{\ell}}[b,i']$ and then

\vfill
$$L_{{\ell+1}}[b,j] = \sigma\left(\left(\sum_{i'} \;W[j,i']\;L_{{\ell}}[b,i']\right) - B[j]\right)$$

\slideplain{Image to Column (Im2C)}

Reduce convolution to matrix multiplication

\vfill
more space but faster.

\vfill
\begin{eqnarray*}
  & & \tilde{L}_{{\ell+1}}[b,x,y,j] \\
  \\
  & = & \left(\sum_{\Delta x, \Delta y, i} W[\Delta x, \Delta y, i, j] *L_{{\ell}}[b,x + \Delta x,\; y + \Delta y,\; i]\right) + B[j]
  \end{eqnarray*}

\vfill
We make a bigger tensor $\tilde{L}$ with two additional indeces.

$$\tilde{L}_{{\ell}}[b,x,y,\Delta x,\Delta y,i] = L_{{\ell}}[b,x+\Delta x,y+\Delta y,i]$$

\slideplain{Image to Column (Im2C)}

\begin{eqnarray*}
  & & \tilde{L}_{{\ell+1}}[b,x,y,j] \\
  \\
  & = & \left(\sum_{\Delta x, \Delta y, i} W[\Delta x, \Delta y, i, j] *L_{{\ell}}[b,x + \Delta x,\; y + \Delta y,\; i]\right) + B[j] \\
  \\
      & = & \left(\sum_{\Delta x, \Delta y, i} \begin{array}{l}
                                              \tilde{L}_{{\ell}}[b,x,y,\Delta x,\Delta y,i]
                                              * W[\Delta x, \Delta y, i, j] \\
  \end{array}\right) + B[j] \\
  \\
    & = & \left(\sum_{(\Delta x, \Delta y, i)} \begin{array}{l}
                                              \tilde{L}_{{\ell}}[(b,x,y),(\Delta x,\Delta y,i)]
                                              * W[(\Delta x, \Delta y, i), j] \\
                                           \end{array}\right) + B[j]
\end{eqnarray*}

\slideplain{Dilation}

A CNN for image classification typically reduces an $N \times N$ image to a single feature vector.

\vfill
Dilation is a trick for treating the whole CNN as a ``filter'' that can be passed over an $M \times M$ image with $M > N$.

\vfill
\centerline{\includegraphics[width = 2.5in]{../images/Convolution}}

\vfill
An output tensor with full spatial dimension can be useful in, for example, image segmentation.

\slide{Dilation}

\centerline{\includegraphics[width=8.0in]{../images/dilation}}

\vfill
This is called a ``fully convolutional'' CNN.

\slide{Dilation}

To implement a fully convolutional CNN we can ``dilate'' the filters by a dilation parameter $d$.

\vfill
\begin{eqnarray*}
\tilde{L}_{{\ell+1}}[b,x,y,j] & = &  W[\Delta x, \Delta y, i, j] L_{{\ell}}[b,x + {\color{red} d*\Delta x}, y + {\color{red} d*\Delta y}, i] + B[j]
\end{eqnarray*}

\slide{Hypercolumns}

An alternative to dilation and fully convolutional networks is hypercolumns.

\vfill
$$L[b,x,y] = L_1[b,x,y];\cdots;L_\ell[b,\floor{x/W_\ell},\floor{y/H_\ell}];\cdots;L_{\cal L}[b]$$

\vfill
where $$L_\ell[b,\floor{x/W_\ell},\floor{y/H_\ell}]\;;\;L_{\ell+1}[b,\floor{x/W_{\ell+1}},\floor{y/H_{\ell+1}}]$$ denotes the concatenation of vectors $$L_\ell[b,\floor{x/W_\ell},\floor{y/H_\ell}]$$ and $$L_{\ell+1}[b,x/W_{\ell+1},y/H_{\ell+1}].$$

\slide{Grouping}

$$L_{\ell+1}[b,x,y] = L^0_{\ell+1}[b,x,y];\cdots;L^{G-1}_{\ell+1}[b,x,y]$$

\vfill
\begin{eqnarray*}
& & L^g_{{\ell+1}}[b,x,y,j] \\
\\
& = & \left(\sum_{\Delta x, \Delta y, i} W^g[\Delta x, \Delta y, i, j] *L_{{\ell}}[b,x + \Delta x,\; y + \Delta y,\; g+i]\right) + B[j]
\end{eqnarray*}

\vfill
For a fixed number of features $j$ in the total output, using $G$ groups reduces the number of weight parameters by a factor of $G$.


\slide{2D CNN in PyTorch}

conv2d({\bf input, weight, bias, stride, padding, dilation, groups})

\bigskip
{\bf input} – tensor (minibatch,in-channels,iH,iW)

\medskip
{\bf weight} – filters (out-channels, in-channels/groups, in-channels,kH,kW)

\medskip
{\bf bias} – tensor (out-channels) . Default: None

\medskip
{\bf stride} – Single number or (sH, sW). Default: 1

\medskip
{\bf padding} – Single number or (padH, padW). Default: 0

\medskip
{\bf dilation} – Single number or (dH, dW). Default: 1

\medskip
{\bf groups} – split input into groups. Default: 1

\slide{Modern Trends}

Modern Convolutions use 3X3 filters.  This is faster and has fewer parameters.  Expressive power is preserved by increasing depth with many stride 1 layers.

\vfill
Max pooling and dilation seem to have disappeared.

\vfill
Resnet and resnet-like architectures are now dominant (next lecture).

\slide{Alexnet}
{\huge
\centerline{Given Input$[227,227,3]$}

\begin{eqnarray*}
L_1[55 \times 55 \times 96] & = & \relu(\conv(\mathrm{Input},\Phi_1,\mathrm{width}\;11,\pad\;0,\stride\;4)) \\
L_2[27 \times 27 \times 96] & = & \mathrm{MaxPool}(L_1,\mathrm{width}\;3,\stride\;2))  \\
L_3[27 \times 27 \times 256] & = & \relu(\conv(L_2,\Phi_3,\mathrm{width}\;5,\pad\;2,\stride\;1))  \\
L_4[13 \times 13 \times 256] & = & \mathrm{MaxPool}(L_3,\mathrm{width}\;3,\stride\;2))  \\
L_5[13 \times 13 \times 384] & = & \relu(\conv(L_4,\Phi_5,\mathrm{width}\;3,\pad\;1,\stride\;1))  \\
L_6[13 \times 13 \times 384] & = & \relu(\conv(L_5,\Phi_6,\mathrm{width}\;3,\pad\;1,\stride\;1))  \\
L_7[13 \times 13 \times 256] & = & \relu(\conv(L_6,\Phi_7,\mathrm{width}\;3,\pad\;1,\stride\;1))  \\
L_8[6 \times 6 \times 256] & = & \mathrm{MaxPool}(L_7,\mathrm{width}\;3,\stride\;2)) \\
L_9[4096] & = & \relu(\mathrm{FC}(L_8,\Phi_9)) \\
L_{10}[4096] & = & \relu(\mathrm{FC}(L_9,\Phi_{10})) \\
s[1000] & = & \relu(\mathrm{FC}(L_{10},\Phi_s)) \;\;\mbox{class scores}
\end{eqnarray*}
}

\slideplain{VGG}
\centerline{\includegraphics[height=4.5in]{../images/VGGStack}}
\centerline{\large Stanford CS231}

\slide{Inception, Google, 2014}

\centerline{\includegraphics[width = 9.0in]{../images/inception1}}
\vfill
\centerline{\includegraphics[width = 3.5in]{../images/inception2}}


\slide{Models for Image Classification in PyTorch}

{\Large
\begin{itemize}

\item AlexNet

\item VGG

\item ResNet

\item SqueezeNet

\item DenseNet

\item Inception v3

\item GoogLeNet

\item ShuffleNet v2

\item MobileNet v2

\item ResNeXt

\item Wide ResNet

\item MNASNet
\end{itemize}
}

\slide{Imagenet Classification}

1000 kinds of objects.

\vfill
\centerline{\includegraphics[height=4.2in]{../images/IVLSRC}}
2016 is 3.0\%, is 2017 2.25\%

\medskip
SOTA as of January 2020 is 1.3\%



\slideplain{END}

}
\end{document}
