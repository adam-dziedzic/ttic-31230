\documentclass{article}
\input ../preamble
\parindent = 0em

%\newcommand{\solution}[1]{}
\newcommand{\solution}[1]{\bigskip {\color{red} {\bf Solution}: #1}}

\begin{document}


\centerline{\bf TTIC 31230 Fundamentals of Deep Learning}

\bigskip

\centerline{\bf Quiz 3}

\bigskip
{\bf Problem 1 (25 points).} Consider the following running update equation.

\begin{eqnarray*}
  y_0  & = & 0 \\
  y_t & = & \left(1 - \frac{1}{N}\right)y_{t-1} + x_t
\end{eqnarray*}

(a) If the input sequence is constant, i.e., if $x_t = c$ for all $t \geq 1$, what is $\lim_{t \rightarrow \infty}\;y_t$?

\solution{

  The limit $y$ must satisfy
  $$y = \left(1-\frac{1}{N}\right)y + c$$
  giving $y = Nc$.
}

\medskip
(b) $y_t$ is a running average of what quantity?

\solution{
  The update can be rewritten as
  $$y_t = \left(1 - \frac{1}{N}\right)y_{t-1} + \frac{1}{N}(Nx_t)$$
  so $y_t$ is the running average of $Nx_t$.
}

\medskip
(c) Express $y_t$ as a function of $\mu_t$ where $\mu_t$ is defined by

\begin{eqnarray*}
  \mu_0  & = & 0 \\
  \mu_t & = & \left(1 - \frac{1}{N}\right)\mu_{t-1} + \frac{1}{N}x_t
\end{eqnarray*}

\solution{
  $y_t$ is the running average of $Nx_t$ which equals $N$ times the running average of $x_t$ so we have
  $$y_t = N \mu_t$$
}

\medskip
{\bf Problem 2 (25 points).} Consider any probability distribution
$P(h)$ over an discrete class ${\cal H}$.  Assume $0 \leq {\cal
  L}(h,x,y) \leq \lmax$. Define
\begin{eqnarray*}
{\cal L}(h) & = & E_{(x,y)\sim \mathrm{Pop}}\;{\cal L}(h,x,y)
\\ \hat{{\cal L}}(h) & = & E_{(x,y)\sim \mathrm{Train}}\;{\cal
  L}(h,x,y)
\end{eqnarray*}
We now have the theorem that with probability at least $1-\delta$ over
the draw of training data the following holds simultaneously for all
$h$.
$${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{\ln \frac{1}{P(h)} + \ln\frac{1}{\delta}}} \;\;\;(1)$$

This motivates
$$h^* = \argmin_h \;\hat{{\cal L}}(h) + \frac{5\lmax}{N} \ln\frac{1}{P(h)}\;\;\;\;(2)$$
The Bayesian maximum a-posteriori (MAP)
rule is
$$h^* = \argmax_h\;P(h)\prod_{(x,y) \in  \mathrm{Train}}\;P(y|x,h)\;\;\;\;(3)$$
For ${\cal L}(h,x,y) = - \ln P(y|x,h)$ (cross entropy loss) rewrite (2) so as to be as similar to
(3) as possible.  Keep in mind that
$$\hat{\cal L}(h) = \frac{1}{N} \sum_{(x,y) \in \mathrm{Train}} -\ln P(y|x,h)$$

\solution{
  \begin{eqnarray*}
    & & \argmin_h \;\left(\frac{1}{N} \sum_{(x,y)\sim \mathrm{Train}}\;-\ln P(y|x,h)\right) + \frac{5\lmax}{N} \ln\frac{1}{P(h)} \\
    \\
    & =  & \argmax_h \;\left(\frac{1}{N} \sum_{(x,y)\sim \mathrm{Train}}\;\ln P(y|x,h)\right) + \frac{5\lmax}{N} \ln P(h) \\
    \\
    & =  & \argmax_h \left(\sum_{(x,y)\sim \mathrm{Train}}\;\ln P(y|x,h)\right) + 5\lmax \ln P(h) \\
    \\
    & =  & \argmax_h \ln \left(P(h)^{5\lmax}\prod_{(x,y)\sim \mathrm{Train}}\; P(y|x,h)\right) \\
    \\
    & = & \argmax_h\;  P(h)^{5\lmax}\prod_{(x,y)\sim \mathrm{Train}}\; P(y|x,h)
  \end{eqnarray*}
}


\eject {\bf Problem 3 (25 points).}

\medskip
(a) Consider a model with $d$ parameters each of which is represented
by a 32 bit floating point number.  Express the bound (1) in problem 2
in terms of the dimension $d$ assuming all representable parameter
vectors are equally likely.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{32d\ln 2 + \ln\frac{1}{\delta}}}$$
}

\medskip
(b) Repeat part (a) but for a model with $d$ parameters represented by
$\Phi_i = z[J[i]]$ where $J[i]$ is an integer index with $0 \leq J[i]
< 32$ and where $z[j]$ is a 32 bit floating point number and where all parameter vectors are equally likely.

\solution{
  $${\cal L}(h) \leq \frac{10}{9}\parens{\hat{\cal L}(h) + \frac{5 L_\mathrm{max}}{N}\parens{(32^2 + 5d)\ln 2 + \ln\frac{1}{\delta}}}$$
}

\bigskip
{\bf Problem 4 (25 points).} This problem is on dynamic programming
for hidden Markov models (HMMs).  Assume we have an input sequence
$x_1,\ldots,x_T$ and a phoneme gold label $y_1,\ldots,y_T$ with $y_t
\in {\cal P}$.  This problem is simpler than CTC because the gold
label has the same length as the input sequence.

\medskip
In an HMM we assume a hidden state sequence $s_1,\ldots,s_T$
with $s_t \in {\cal S}$ where ${\cal S}$ is some finite sets of ``hidden states''.  Here will assume that then
some deep network has computed transition probabilities and emission probabilities.
$$P_{\mathrm{Trans}}(s_{t+1}\;|\; s_t)$$
$$P_{\mathrm{Emit}}(y_t\;|\;s_t)$$
We assume an initial state $s_{\mathrm{init}}$ and a stop state $s_{\mathrm{stop}}$ such that $s_1= s_{\mathrm{init}}$
(before emitting any phonemes). The length $T$ is determined
by when the hidden state becomes $s_{\mathrm{stop}}$ giving $s_{T+1} = s_{\mathrm{stop}}$. 

\medskip
For a given gold sequence $y_1,\ldots,y_T$ we define a ``forward tensor'' as
$$F[t,s] = P(y_1,\ldots,y_{t-1}\;\wedge s_t = s)$$
We have
\begin{eqnarray*}
  F[1,s_{\mathrm{init}}] & = & 1 \\
  F[1,s] & = & 0 \;\;\;\mbox{for $s  \not = s_{\mathrm{init}}$}
\end{eqnarray*}

\medskip
(a) Write a dynamic programming equation to compute $F[t,s]$ from $F[t-1,s']$ for various values of $s'$.

\solution{
  $$F[t,s] = \sum_{s'}\;F[t-1,s']P_{\mathrm{Emit}}(y_{t-1}|s')P_{\mathrm{Trans}}(s|s')$$
}


\medskip
(b) Express $P(y_1,\ldots,y_T)$ in terms of $F[t,s]$.

\solution{
  $$P(y_1,\ldots y_T) = F[T+1,s_{\mathrm{stop}}]$$
}

\medskip
(c) Explain why, if the forward equations are written in a framework,
we do not need to also implement ``backward'' equations to compute
$$B[t,s] = P(y_t,\ldots,y_T\;|\;s_t=s).$$

\solution{
  Once we have expressed the loss $- \ln P(y_1,\ldots,y_T)$ in a framework we can train the model by SGD using the framework's implementation of back-propagation.
  Nothing more is needed.
}

\end{document}
